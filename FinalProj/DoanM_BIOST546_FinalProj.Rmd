---
title: "BIOST 546 Final Project"
author: "My-Anh Doan"
date: "2023-01-25"
output:
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, results = "hide", warning = FALSE, message = FALSE}
# set global options for code chunks
knitr::opts_chunk$set(message = FALSE, warning = FALSE, collapse = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

library(dplyr)
library(knitr)
library(ggplot2)
library(caret)
library(glmnet)
library(pROC)
library(randomForest)
library(ranger)

```

```{r load-data}
# load data
load("./dataset/ADProj.RData")
str(ADProj, max.level = 1)

X_train <- ADProj[[1]]
y_train <- ADProj[[2]]
X_test <- ADProj[[3]]

train_dat <- data.frame(X_train, y_train)
contrasts(train_dat$Outcome)

# check for missing values
which(complete.cases(train_dat) == FALSE)

# how many observations in each class in the training data
kable(train_dat %>% count(Outcome), caption = "# of observations in training diagnosis outcomes")

```
The training data set above contains $n$ = 400 observations and $p$ = 360 predictors/features.
In the training data set, there number of observations for each diagnosis class ("C" or "AD") are listed in Table 1.

```{r simple-glm-model}
glm_model <- glm(formula = Outcome ~ .,
                 family = binomial(link = "logit"),
                 data = train_dat)

# training data
glm_prob_train <- predict(glm_model, type = "response", train_dat)
glm_label_train <- ifelse(glm_prob_train > 0.5, "AD", "C")

glm_train_matrix <- confusionMatrix(factor(glm_label_train,
                                           levels = c("AD","C")),
                                    factor(train_dat$Outcome,
                                           levels = c("AD","C")),
                                    positive = "AD")
kable(glm_train_matrix$table)

glm_acc <- glm_train_matrix$overall[1]
glm_acc

# test data
glm_prob_test <- predict(glm_model, type = "response", X_test)
glm_label_test <- ifelse(glm_prob_test > 0.5, "AD", "C")

test_counts <- glm_label_test %>%
  factor() %>%
  as.data.frame() %>%
  rename(Outcome = 1) %>%
  count(Outcome)

kable(test_counts, caption = "glm model: # of observations in test diagnosis outcomes")

write.table(glm_label_test, "./FinalProj/DoanM_Pred1.txt",
            row.names = FALSE, col.names = FALSE, quote = FALSE)
```


```{r ridge-v-lasso-glm-model}
# split training data into train/validation sets
set.seed(2)
random_sample <- sample(1:nrow(X_train), size = 0.75 * nrow(X_train),
                        replace = FALSE)

X_train_2 <- X_train[random_sample, ]
y_train_2 <- as.factor(y_train$Outcome[random_sample])

X_test_2 <- X_train[-random_sample, ]
y_test_2 <- as.factor(y_train$Outcome[-random_sample])

as.data.frame(y_train_2) %>% count(y_train_2)

# normalize data
X_train_scaled <- scale(X_train_2)
X_test_scaled <- scale(X_test_2)

# ridge glm model ----
lambda_grid <- 10^seq(5, -18, length = 100)

ridge_model <- glmnet(X_train_scaled, y_train_2, alpha = 0,
                      lambda = lambda_grid, family = "binomial")

# obtain optimal lambda value
ridge_model_cv <- cv.glmnet(X_train_scaled, y_train_2, alpha = 0,
                            nfolds = 10, lambda = lambda_grid,
                            family = "binomial", type.measure = "class")
plot(ridge_model_cv)

optimal_ridge_lambda <- ridge_model_cv$lambda.min

# Training predictions, confusion table, and prediction accuracy
ridge_opt_train <- glmnet(X_train_scaled, y_train_2, alpha = 0,
                          lambda = optimal_ridge_lambda, family = "binomial")

ridge_train_prob <- predict(ridge_opt_train, type = "response", X_train_scaled)
ridge_train_class <- ifelse(ridge_train_prob > 0.5, "AD", "C")

ridge_train_matrix <- confusionMatrix(factor(ridge_train_class,
                                             levels = c("AD", "C")),
                                      y_train_2, positive = "AD")

kable(ridge_train_matrix$table,
      caption = "Ridge Logistic Regression Training Set Confusion Matrix")

# Test predictions, confusion table, and prediction accuracy
ridge_test_prob <- predict(ridge_opt_train, type = "response", X_test_scaled)
ridge_test_class <- ifelse(ridge_test_prob > 0.5, "AD", "C")

ridge_test_matrix <- confusionMatrix(factor(ridge_test_class,
                                            levels = c("AD", "C")),
                                     y_test_2, positive = "AD")
kable(ridge_test_matrix$table,
      caption = "Ridge Logistic Regression Test Set Confusion Matrix")

ridge_roc_score <- roc(response = y_test_2, predictor = ridge_test_prob)
ridge_roc_score$auc

# lasso glm model ----
lasso_model <- glmnet(X_train_scaled, y_train_2, alpha = 1,
                      lambda = lambda_grid, family = "binomial")

lasso_train_cv <- cv.glmnet(X_train_scaled, y_train_2, alpha = 1,
                            nfolds = 10, lambda = lambda_grid,
                            family = "binomial", type.measure = "class")
plot(lasso_train_cv)

optimal_lasso_lambda <- lasso_train_cv$lambda.min

# Training predictions, confusion table, and prediction accuracy
lasso_opt_train <- glmnet(X_train_scaled, y_train_2, alpha = 1,
                          lambda = optimal_lasso_lambda, family = "binomial")

lasso_train_prob <- predict(lasso_opt_train, type = "response", X_train_scaled)
lasso_train_class <- ifelse(lasso_train_prob > 0.5, "AD", "C")

lasso_train_matrix <- confusionMatrix(factor(lasso_train_class,
                                             levels = c("AD","C")),
                                      y_train_2, positive = "AD")

# Test predictions, confusion table, and prediction accuracy
lasso_test_prob <- predict(lasso_opt_train, type = "response", X_test_scaled)
lasso_test_class <- ifelse(lasso_test_prob > 0.5, "AD", "C")

lasso_test_matrix <- confusionMatrix(factor(lasso_test_class,
                                            levels = c("AD","C")),
                                     y_test_2, positive = "AD")

lasso_roc_score <- roc(response = y_test_2, predictor = lasso_test_prob)
lasso_roc_score$auc

# selection between lasso or ridge ----
kable(data.frame(Model = c("ridge", "lasso"),
           `Training accuracy` = c(ridge_train_matrix$overall[1],
                                   lasso_train_matrix$overall[1]),
           `Test accuracy` = c(ridge_test_matrix$overall[1],
                               lasso_test_matrix$overall[1]),
           `Test AUC` = c(ridge_roc_score$auc,
                          lasso_roc_score$auc),
           `Non-zero coefficients` = c(sum(predict(ridge_model,
                                                   s = optimal_ridge_lambda,
                                                   type = "coefficients")[-1, ] != 0),
                                       sum(predict(lasso_model,
                                                   s = optimal_lasso_lambda,
                                                   type = "coefficients")[-1, ] != 0)),
           check.names = FALSE),
      caption = "glm Model Performance Summary")

# apply ridge model to actual test data ----
ridge_prob_test <- predict(ridge_opt_train, type = "response", as.matrix(X_test))
ridge_label_test <- ifelse(ridge_prob_test > 0.5, "AD", "C")

test_counts <- ridge_label_test %>%
  factor() %>%
  as.data.frame() %>%
  rename(Outcome = 1) %>%
  count(Outcome)

kable(test_counts, caption = "Ridge glm model: # of observations in test diagnosis outcomes")

write.table(ridge_label_test, "./FinalProj/DoanM_Pred2.txt",
            row.names = FALSE, col.names = FALSE, quote = FALSE)

```


```{r random-forest}
# divide data into K subsets for repeated K-fold cross-validation during training
# use K = 10 folds and repetitions = 3
set.seed(1)
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

rf_model <- ranger(Outcome ~ ., data = train_dat,
                   importance = "permutation",
                   classification = TRUE)

pred_rf <- predict(rf_model,
                   data = X_test,
                   type = "response")

rf_test_counts <- pred_rf$predictions %>%
  factor() %>%
  as.data.frame() %>%
  rename(Outcome = 1) %>%
  count(Outcome)

kable(rf_test_counts, caption = "Random Forest model: # of observations in test diagnosis outcomes")

write.table(pred_rf$predictions, "./FinalProj/DoanM_Pred3.txt",
            row.names = FALSE, col.names = FALSE, quote = FALSE)
```